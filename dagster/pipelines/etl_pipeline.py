from dagster import Definitions, job, op, get_dagster_logger, In, Out
import pandas as pd
import boto3
import duckdb
from io import BytesIO
from sqlalchemy import create_engine, text

# -----------------------------
# √âtape 1 & 2 : MinIO ‚Üí DuckDB
# -----------------------------

@op
def load_from_minio():
    logger = get_dagster_logger()
    s3 = boto3.client(
        "s3",
        endpoint_url="http://minio:9000",
        aws_access_key_id="minioadmin",
        aws_secret_access_key="minioadmin",
    )
    try:
        obj = s3.get_object(Bucket="datatest", Key="bareme_solde.xlsx")
        df = pd.read_excel(BytesIO(obj["Body"].read()))
        logger.info("‚úÖ Fichier charg√© depuis MinIO.\nAper√ßu :\n" + df.head().to_string())
        return df
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement depuis MinIO : {e}")
        raise

@op
def count_rows_minio(df: pd.DataFrame) -> int:
    count = len(df)
    get_dagster_logger().info(f"üî¢ MinIO contient {count} lignes.")
    return count

@op
def transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    logger = get_dagster_logger()
    original_len = len(df)

    # üîπ Supprimer les doublons
    df_clean = df.drop_duplicates()
    after_dedup = len(df_clean)

    # üîπ Supprimer les lignes contenant des valeurs nulles
    df_clean = df_clean.dropna()
    after_dropna = len(df_clean)

    # üîπ Tenter un tri automatique
    tri_colonnes = ["matricule", "id", "ID", "code", "date"]  # √† adapter selon ton fichier
    colonne_de_tri = next((col for col in tri_colonnes if col in df_clean.columns), None)

    if colonne_de_tri:
        df_clean = df_clean.sort_values(by=colonne_de_tri)
        logger.info(f"üîΩ Donn√©es tri√©es par la colonne '{colonne_de_tri}'.")
    else:
        logger.warning("‚ö†Ô∏è Aucune colonne trouv√©e pour trier les donn√©es.")

    # üîπ Log d‚Äôaper√ßu
    logger.info(
        f"""üßº Nettoyage termin√© :
        - Lignes d'origine : {original_len}
        - Apr√®s suppression des doublons : {after_dedup}
        - Apr√®s suppression des valeurs nulles : {after_dropna}
        - Colonnes pr√©sentes : {list(df_clean.columns)}"""
            )

    return df_clean

@op
def write_to_duckdb(df: pd.DataFrame) -> bool:
    logger = get_dagster_logger()
    try:
        con = duckdb.connect("/opt/dagster/dagster_home/bareme_solde.duckdb")
        con.execute("DROP TABLE IF EXISTS bareme_solde")
        con.execute("CREATE TABLE bareme_solde AS SELECT * FROM df")
        tables = con.execute("SHOW TABLES").fetchall()
        logger.info("‚úÖ Table √©crite dans DuckDB. Tables pr√©sentes : " + str(tables))
        logger.info("‚úÖ Table √©crite dans DuckDB avec succ√®s.")
        logger.info("üìÑ Liste des tables dans DuckDB : " + str(con.execute("SHOW TABLES").fetchall()))
        con.close()
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l‚Äô√©criture dans DuckDB : {e}")
        raise
    return True  # juste pour d√©clencher la d√©pendance

@op
def load_from_duckdb(trigger: bool) -> pd.DataFrame:
    logger = get_dagster_logger()
    try:
        con = duckdb.connect("/opt/dagster/dagster_home/bareme_solde.duckdb")
        tables = con.execute("SHOW TABLES").fetchall()
        logger.info("üìÑ Tables disponibles dans DuckDB : " + str(tables))
        if ('bareme_solde',) not in tables:
            raise Exception("‚ùå La table 'bareme_solde' n'existe pas dans DuckDB.")
        df = con.execute("SELECT * FROM bareme_solde").df()
        logger.info("‚úÖ Donn√©es charg√©es depuis DuckDB.")
        con.close()
        return df
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la lecture depuis DuckDB : {e}")
        raise

@op
def count_rows_duckdb(df: pd.DataFrame) -> int:
    count = len(df)
    get_dagster_logger().info(f"üî¢ DuckDB contient {count} lignes.")
    return count

# -----------------------------
# √âtape 3 : PostgreSQL
# -----------------------------

@op
def write_to_postgres(df: pd.DataFrame):
    logger = get_dagster_logger()
    try:
        engine = create_engine("postgresql://admin:admin@postgres:5432/datalake")
        with engine.connect() as conn:
            df.to_sql("bareme_solde", conn, if_exists="replace", index=False)
        logger.info("‚úÖ Donn√©es ins√©r√©es dans PostgreSQL.")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'insertion PostgreSQL : {e}")
        raise

@op
def read_from_postgres():
    logger = get_dagster_logger()
    try:
        engine = create_engine("postgresql://admin:admin@postgres:5432/datalake")
        with engine.connect() as conn:
            # V√©rifier que la table existe
            result = conn.execute(text("SELECT to_regclass('public.bareme_solde')"))
            if result.scalar() is None:
                raise Exception("‚ùå La table 'bareme_solde' n'existe pas dans PostgreSQL.")
            df = pd.read_sql("SELECT * FROM bareme_solde", conn)
        logger.info("üì• Lecture depuis PostgreSQL r√©ussie.\nAper√ßu :\n" + df.head().to_string())
        return df
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la lecture PostgreSQL : {e}")
        raise

@op
def read_count_from_postgres():
    logger = get_dagster_logger()
    try:
        engine = create_engine("postgresql://admin:admin@postgres:5432/datalake")
        with engine.connect() as conn:
            result = conn.execute(text("SELECT to_regclass('public.bareme_solde')"))
            if result.scalar() is None:
                raise Exception("‚ùå La table 'bareme_solde' n'existe pas dans PostgreSQL.")
            result = conn.execute(text("SELECT COUNT(*) FROM bareme_solde"))
            count = result.scalar()
        logger.info(f"üìä Il y a {count} lignes dans PostgreSQL.")
        return count
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du comptage PostgreSQL : {e}")
        raise


# -----------------------------
# √âtape 4 : Validation
# -----------------------------

@op
def validate_counts(minio_count: int, duckdb_count: int, postgres_count: int):
    logger = get_dagster_logger()
    if minio_count == duckdb_count == postgres_count:
        logger.info("‚úÖ Validation r√©ussie : tous les syst√®mes contiennent le m√™me nombre de lignes.")
    else:
        logger.error(f"‚ùå Incoh√©rence : MinIO={minio_count}, DuckDB={duckdb_count}, PostgreSQL={postgres_count}")
        raise Exception("Les comptes ne correspondent pas.")

# -----------------------------
# D√©finition des jobs Dagster
# # -----------------------------

# @job
# def full_pipeline_job():
#     df = load_from_minio()
#     df_clean = transform_dataframe(df)  # üßº √©tape de nettoyage
#     count_minio = count_rows_minio(df_clean)

#     duckdb_written = write_to_duckdb(df_clean)
#     df_duckdb = load_from_duckdb(duckdb_written)

#     count_duckdb = count_rows_duckdb(df_duckdb)
#     write_to_postgres(df_duckdb)
#     postgres_count = read_count_from_postgres()

#     validate_counts(count_minio, count_duckdb, postgres_count)
@job
def full_pipeline_job():
    df = load_from_minio()
    raw_count = count_rows_minio(df)        # <--- ici : donn√©es brutes
    df_clean = transform_dataframe(df)
    duckdb_written = write_to_duckdb(df_clean)
    df_duckdb = load_from_duckdb(duckdb_written)
    count_duckdb = count_rows_duckdb(df_duckdb)
    write_to_postgres(df_duckdb)
    postgres_count = read_count_from_postgres()

    validate_counts(count_duckdb, count_duckdb, postgres_count)  # comparer post-nettoyage


@job
def test_postgres_write_and_read():
    df = load_from_minio()
    write_to_postgres(df)
    read_from_postgres()
    read_count_from_postgres()



# -----------------------------
# Export du job principal
# -----------------------------

etl_job = full_pipeline_job
# -----------------------------


#Cr√©er un asset simple (bareme_solde_asset) qui repr√©sente le chargement des donn√©es depuis MinIO, 
# et programmer un schedule qui ex√©cute le pipeline (full_pipeline_job) automatiquement chaque jour
#Un asset (actif de donn√©es) est une repr√©sentation d‚Äôune donn√©e concr√®te 
# ou d‚Äôun fichier (comme une table dans une base de donn√©es, 
# un fichier dans MinIO, etc.) qu‚Äôon peut produire ou transformer dans un pipeline
from dagster import asset, define_asset_job, ScheduleDefinition

@asset
def bareme_solde_asset() -> pd.DataFrame:
    logger = get_dagster_logger()
    s3 = boto3.client(
        "s3",
        endpoint_url="http://minio:9000",
        aws_access_key_id="minioadmin",
        aws_secret_access_key="minioadmin",
    )
    try:
        obj = s3.get_object(Bucket="datatest", Key="bareme_solde.xlsx")
        df = pd.read_excel(BytesIO(obj["Body"].read()))
        logger.info("\U0001F4E6 Asset charg√© depuis MinIO. Aper√ßu :\n" + df.head().to_string())
        return df
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement de l'asset : {e}")
        raise

# D√©finir un job bas√© sur l'asset
bareme_job = define_asset_job(name="bareme_job", selection=["bareme_solde_asset"])

#Un schedule est un d√©clencheur automatique bas√© sur le temps
#si on veut que  pipeline ETL s'ex√©cute automatiquement tous les jours 
# pour r√©cup√©rer les nouvelles donn√©es depuis MinIO et 
# les injecter dans DuckDB/Postgres, on utilise schedule
# Planification quotidienne √† 07h00 (UTC)
daily_schedule = ScheduleDefinition(
    job=bareme_job,
    cron_schedule="0 7 * * *",  # tous les jours √† 7h00
)  





